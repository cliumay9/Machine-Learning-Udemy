6-3
2+5
winners <- read.delim(pipe('pbpaste'))
View(winners)
View(winners)
winners
winners <- read.delim(pipe('pbpaste'))
View(winners)
winners
x+y
x<- 1
y<-2
x+y
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
Oscar
Romeo+Oscar
Calvin<-Romeo+Oscar
Calvin<-x
Oscar
source('~/.active-rstudio-document', echo=TRUE)
Juliet-Romeo
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/Downloads/Ex_Files_Intro_Data_Science/DataScienceExcercise_github/Ensembles with Random Forest.R')
source('~/.active-rstudio-document')
install.packages("randomForest")
install.packages("party")  # Install party
library(datasets)          # Load built-in datasets
library(party)             # Load party
head(iris)
iris.ct <- ctree(Species~., data=iris)
plot(iris.ct)
table(predict(iris.ct),iris$Species)
head(mtcars)
car.rt<-ctree(mpg~cyl+disp+hp+drat+wt+qsec, data=mtcars)
plot(car.rt2)
plot(car.rt3)
plot(car.rt)
table(predict(car.rt),mtcars$mpg)
data(HouseVotes84, package = "mlbench")
vote <- HouseVotes84
head(vote)
set.seed(1984)
set.seed(1984)
vote.split <- sample(2, nrow(vote),
replace = TRUE,
prob = c(2/3, 1/3))
vote.train <- vote[vote.split == 1, ]
vote.test  <- vote[vote.split == 2, ]
# ==========================================================
# Build and test classifier
# nbc= naive Bayes Classifier
# ==========================================================
nbc <- naiveBayes(Class ~ ., data = vote.train)
install.packages("e1071")
install.packages("mlbench")
library(e1071)
library(mlbench)
# ==========================================================
# Prepare data: HouseVotes84
# Using "HouseVotes84" from mlbench package
# ==========================================================
data(HouseVotes84, package = "mlbench")
vote <- HouseVotes84
head(vote)
# Split data into training set (2/3) and testing set (1/3)
# set.seed()to have consistency
# Trying to categorize whether one person is democrat or republican
# Split data with 2 sets: a training set and a testing set (WITH vote.split)
set.seed(1984)
vote.split <- sample(2, nrow(vote),
replace = TRUE,
prob = c(2/3, 1/3))
vote.train <- vote[vote.split == 1, ]
vote.test  <- vote[vote.split == 2, ]
head(vote.train)
head(vote.test)
nbc <- naiveBayes(Class ~ ., data = vote.train)
nbc  # Examine the classifier
table(predict(nbc, vote.train[, -1]), vote.train[, 1])
round(prop.table(table(predict(nbc, vote.train[, -1]),
vote.train[, 1]),
1),  # Row percentages (2 = column)
2) * 100        # Round to 2 digits, remove decimals
table(predict(nbc, vote.test[, -1]), vote.test[, 1])
round(prop.table(table(predict(nbc, vote.test[, -1]),
vote.test[, 1]),
1),  # Row percentages (2 = column)
2) * 100        # Round to 2 digits, remove decimals
rm(list = ls())
detach("package:e1071", unload = TRUE)
detach("package:mlbench", unload = TRUE)
cat("\014")
install.packages('neuralnet')  # Install neuralnet
library(neuralnet)             # Load neuralnet
set.seed(1202)                             # Set seed
train.x <- sample(1:100, 50, replace = T)
head(train.x)
train.x
train.y <- sqrt(train.x)
train y
train.y
train.xy <- as.data.frame(cbind(train.x, train.y))
head(train.xy)
net.sqrt <- neuralnet(train.y ~ train.x,
train.xy,
hidden = 10,
threshold = 0.01)
plot(net.sqrt)
hidden = 20,
net.sqrt <- neuralnet(train.y ~ train.x,
train.xy,
hidden = 20,
threshold = 0.01)
plot(net.sqrt)
test.x <- as.data.frame((1:10)^2)
test.y <- compute(net.sqrt, test.x)
test.y
test.x
test.y
net.table <- cbind(test.x,
sqrt(test.x),
round(test.y$net.result, 2))
net.table
net.table <- cbind(test.x,
sqrt(test.x),
round(test.y$net.result, 3))
colnames(net.table) <- c("X","Y-Exp","Y-ANN")
net.table  # Display table
test.trainx <- comput(net.sqrt, train.x)
test.trainx <- compute(net.sqrt, train.x)
test.trainx
train.y
#
test.trainx
train.y
net.table<- cbind(train.x, train.y, round(test.trainx$net.result,3))
net.table1<- cbind(train.x, train.y, round(test.trainx$net.result,3))
net.table <- cbind(test.x,
sqrt(test.x),
round(test.y$net.result, 3))
net.table1
net.table <- cbind(test.x,
sqrt(test.x),
round(test.y$net.result, 3))
colnames(net.table) <- c("X","Y-Exp","Y-ANN")
#Rename the column name
net.table  # Display table
net.table
test.y
print("Hello World!")
print("Hello World")
setwd("~/")
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
View(dataset)
# Logistic Regression
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
library(caTools)
setseed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
# Logistic Regression
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
# Feature Scaling
training_set[,3:5] = scale(training_set[,3:5])
test_set[,3:5] = scale(test_set[,3:5])
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
View(test_set)
training_set[,1:2] = scale(training_set[,1:2])
View(training_set)
test_set[,1:2] = scale(test_set[,1:2])
# Feature Scaling
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
View(test_set)
View(test_set)
View(training_set)
classifier = glm(Purchased~., data=training_set)
classifier = glm(Purchased~.,
family = binomial,
data=training_set)
classifier = glm(Purchased~.,
family = binomial,
data = training_set)
prob_pred = predict(classifier, type = 'response', newdata = testset[,-3])
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-3])
prob_pred
source('~/Desktop/RevJet/revjet_DataPreprocessing.R')
# Building Logistic Classification Model
Lclassifier = glm(CTR~.,
family = binomial,
data = training_set)
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-10])
View(training_set)
source('~/.active-rstudio-document')
# Building Logistic Classification Model
Lclassifier = glm(CTR~.,
family = binomial,
data = training_set)
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-10])
Lclassifier = glm(CTR~.,
family = binomial,
data = training_set)
prob_pred = predict(Lclassifier, type = 'response', newdata = test_set[,-10])
Lclassifier = glm(CTR~.,
family = binomial,
data = training_set)
View(training_set)
View(d)
View(rawdataset)
View(dataset)
source('~/.active-rstudio-document')
View(training_set)
Lclassifier = glm(CTR~.,
family = binomial,
data = training_set)
prob_pred = predict(Lclassifier, type = 'response', newdata = test_set[,-10])
prob_pred
y_pred = ifelse(prob_pred>=0.5, 1, 0)
# Logistic Regression
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
# Feature Scaling
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
## Creating Logistic classifier
classifier = glm(Purchased~.,
family = binomial,
data = training_set)
#Predicting the test result
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-3])
# response response vector
y_pred = ifelse(prob_pred>=0.5, 1, 0)
y_pred
View(test_set)
View(training_set)
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-3])
# Logistic Regression
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split = FALSE)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
## Creating Logistic classifier
classifier = glm(Purchased~.,
family = binomial,
data = training_set)
#Predicting the test result
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-3])
# response response vector
y_pred = ifelse(prob_pred>=0.5, 1, 0)
y_pred
cm= table(test_set[,3],y_pred)
cm
# Logistic Regression
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Logistic_Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
## Creating Logistic classifier
classifier = glm(Purchased~.,
family = binomial,
data = training_set)
#Predicting the test result
prob_pred = predict(classifier, type = 'response', newdata = test_set[,-3])
# response response vector
y_pred = ifelse(prob_pred>=0.5, 1, 0)
#Building Confusion matrix
cm= table(test_set[,3],y_pred)
cm
set = training_set
X1= seq(min(set[,1])-1, max(set[,1])+1, by =0.01)
X2 = seq(min(set[,2])-1, max(set[,2])+1, by =0.01)
# install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1= seq(min(set[,1])-1, max(set[,1])+1, by =0.01)
X2 = seq(min(set[,2])-1, max(set[,2])+1, by =0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set)=c('Age','EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata= grid_set)
y_grid = ifelse(prob_set>0.5, 1, 0)
library(ElemStatLearn)
set = training_set
X1= seq(min(set[,1])-1, max(set[,1])+1, by =0.01)
X2 = seq(min(set[,2])-1, max(set[,2])+1, by =0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set)=c('Age','EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata= grid_set)
y_grid = ifelse(prob_set>0.5, 1, 0)
plot(set[,-3],
main = "Logistic Regression (Training Set)",
xlab = 'Age', ylab='Estimated Salary',
xlim = range(X1), ylim=range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add= TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid==1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3]==1, 'green4','red3'))
set = test_set
X1= seq(min(set[,1])-1, max(set[,1])+1, by =0.01)
X2 = seq(min(set[,2])-1, max(set[,2])+1, by =0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set)=c('Age','EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata= grid_set)
y_grid = ifelse(prob_set>0.5, 1, 0)
plot(set[,-3],
main = "Logistic Regression (Test Set)",
xlab = 'Age', ylab='Estimated Salary',
xlim = range(X1), ylim=range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add= TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid==1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3]==1, 'green4','red3'))
setwd("~/Desktop/Ex_Files_DSF_DataMining/Machine-Learning-Udemy/Hierarchical Clustering")
dataset = read.csv('Mall_Customers.csv')
View(dataset)
View(dataset)
View(dataset)
X = dataset[4:5]
View(X)
dendrogram = hclust(dist(X, method ='euclidean'), method = 'ward.D')
plot(dendrogram,
main = paste('Dendrogram'),
xlab = 'Customers',
ylab = 'Euclidean Distances')
# Fittng HC model into the dataset
hc = hclust(dist(X, method ='euclidean'), method = 'ward.D')
y_hc = cutree(hc, 5)
y_hc
library(cluster)
clusplot(X,y_hc, lines = 0,shade =TRUE, color = TRUE,labels = 2, plotchar = FALSE,span = TRUE, main=paste('Clusters of clients'),xlab ="Annual Income", ylab ="Spending Score")
# Visualizing the dataset
library(cluster)
clusplot(X,y_hc, lines = 0,shade =TRUE, color = TRUE,labels = 2, plotchar = FALSE,span = TRUE, main=paste('Clusters of clients'),xlab ="Annual Income", ylab ="Spending Score")
